{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.333333333333333,
  "eval_steps": 500,
  "global_step": 40,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.13,
      "grad_norm": 1.38254976272583,
      "learning_rate": 4.964285714285715e-05,
      "loss": 3.2811,
      "step": 1
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.9757725596427917,
      "learning_rate": 4.928571428571429e-05,
      "loss": 3.0637,
      "step": 2
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.1508888006210327,
      "learning_rate": 4.892857142857143e-05,
      "loss": 3.7356,
      "step": 3
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.4434536695480347,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 3.2042,
      "step": 4
    },
    {
      "epoch": 0.67,
      "grad_norm": 1.5605518817901611,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 3.4157,
      "step": 5
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.2776076793670654,
      "learning_rate": 4.785714285714286e-05,
      "loss": 3.0715,
      "step": 6
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.4082485437393188,
      "learning_rate": 4.75e-05,
      "loss": 3.7044,
      "step": 7
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.9951032400131226,
      "learning_rate": 4.714285714285714e-05,
      "loss": 3.169,
      "step": 8
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.083357572555542,
      "learning_rate": 4.678571428571429e-05,
      "loss": 3.4841,
      "step": 9
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.9282803535461426,
      "learning_rate": 4.642857142857143e-05,
      "loss": 3.1673,
      "step": 10
    },
    {
      "epoch": 1.47,
      "grad_norm": 1.1684409379959106,
      "learning_rate": 4.607142857142857e-05,
      "loss": 2.858,
      "step": 11
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.217862606048584,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 2.7849,
      "step": 12
    },
    {
      "epoch": 1.73,
      "grad_norm": 1.031886100769043,
      "learning_rate": 4.5357142857142856e-05,
      "loss": 2.9723,
      "step": 13
    },
    {
      "epoch": 1.87,
      "grad_norm": 1.0826327800750732,
      "learning_rate": 4.5e-05,
      "loss": 3.3107,
      "step": 14
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.183716058731079,
      "learning_rate": 4.464285714285715e-05,
      "loss": 3.07,
      "step": 15
    },
    {
      "epoch": 2.13,
      "grad_norm": 1.3884243965148926,
      "learning_rate": 4.428571428571428e-05,
      "loss": 3.0803,
      "step": 16
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.1425530910491943,
      "learning_rate": 4.392857142857143e-05,
      "loss": 2.861,
      "step": 17
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.9518985748291016,
      "learning_rate": 4.3571428571428576e-05,
      "loss": 2.9983,
      "step": 18
    },
    {
      "epoch": 2.53,
      "grad_norm": 1.1768567562103271,
      "learning_rate": 4.3214285714285716e-05,
      "loss": 3.476,
      "step": 19
    },
    {
      "epoch": 2.67,
      "grad_norm": 1.721688151359558,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 3.1463,
      "step": 20
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.9595885872840881,
      "learning_rate": 4.25e-05,
      "loss": 2.6421,
      "step": 21
    },
    {
      "epoch": 2.93,
      "grad_norm": 1.0860456228256226,
      "learning_rate": 4.214285714285714e-05,
      "loss": 3.3575,
      "step": 22
    },
    {
      "epoch": 3.07,
      "grad_norm": 1.015987515449524,
      "learning_rate": 4.178571428571429e-05,
      "loss": 3.1912,
      "step": 23
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.1665496826171875,
      "learning_rate": 4.1428571428571437e-05,
      "loss": 2.4571,
      "step": 24
    },
    {
      "epoch": 3.33,
      "grad_norm": 1.2942067384719849,
      "learning_rate": 4.107142857142857e-05,
      "loss": 3.3926,
      "step": 25
    },
    {
      "epoch": 3.47,
      "grad_norm": 1.5730509757995605,
      "learning_rate": 4.0714285714285717e-05,
      "loss": 3.2799,
      "step": 26
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.9992223978042603,
      "learning_rate": 4.035714285714286e-05,
      "loss": 2.8164,
      "step": 27
    },
    {
      "epoch": 3.73,
      "grad_norm": 1.0717445611953735,
      "learning_rate": 4e-05,
      "loss": 2.9064,
      "step": 28
    },
    {
      "epoch": 3.87,
      "grad_norm": 1.2215021848678589,
      "learning_rate": 3.964285714285714e-05,
      "loss": 3.606,
      "step": 29
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.4023139476776123,
      "learning_rate": 3.928571428571429e-05,
      "loss": 3.1802,
      "step": 30
    },
    {
      "epoch": 4.13,
      "grad_norm": 1.215972900390625,
      "learning_rate": 3.892857142857143e-05,
      "loss": 3.0379,
      "step": 31
    },
    {
      "epoch": 4.27,
      "grad_norm": 1.1688423156738281,
      "learning_rate": 3.857142857142858e-05,
      "loss": 2.3985,
      "step": 32
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.1406762599945068,
      "learning_rate": 3.821428571428572e-05,
      "loss": 3.167,
      "step": 33
    },
    {
      "epoch": 4.53,
      "grad_norm": 1.435353398323059,
      "learning_rate": 3.785714285714286e-05,
      "loss": 2.9587,
      "step": 34
    },
    {
      "epoch": 4.67,
      "grad_norm": 1.3516274690628052,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 3.1041,
      "step": 35
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.0434650182724,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 2.5742,
      "step": 36
    },
    {
      "epoch": 4.93,
      "grad_norm": 1.1608375310897827,
      "learning_rate": 3.678571428571429e-05,
      "loss": 3.4106,
      "step": 37
    },
    {
      "epoch": 5.07,
      "grad_norm": 1.3949471712112427,
      "learning_rate": 3.642857142857143e-05,
      "loss": 3.3886,
      "step": 38
    },
    {
      "epoch": 5.2,
      "grad_norm": 1.345047950744629,
      "learning_rate": 3.607142857142857e-05,
      "loss": 3.5187,
      "step": 39
    },
    {
      "epoch": 5.33,
      "grad_norm": 1.1470110416412354,
      "learning_rate": 3.571428571428572e-05,
      "loss": 2.817,
      "step": 40
    }
  ],
  "logging_steps": 1,
  "max_steps": 140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 20,
  "total_flos": 316689174872064.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
